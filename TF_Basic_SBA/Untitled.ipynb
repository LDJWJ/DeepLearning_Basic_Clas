{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install utils\n",
    "# !pip show requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TextLoader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-674e4814be40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# 학습에 필요한 설정값들을 지정합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TextLoader'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Char-RNN 예제\n",
    "# Author : solaris33\n",
    "# Project URL : http://solarisailab.com/archives/2487\n",
    "# GitHub Repository : https://github.com/solaris33/char-rnn-tensorflow/\n",
    "# Reference : https://github.com/sherjilozair/char-rnn-tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from utils import TextLoader\n",
    "\n",
    "# 학습에 필요한 설정값들을 지정합니다.\n",
    "data_dir = 'data/tinyshakespeare' # 셰익스피어 희곡 <리처드 3세> 데이터로 학습\n",
    "#data_dir = 'data/linux' # <Linux 소스코드> 데이터로 학습\n",
    "batch_size = 50 # Training : 50, Sampling : 1\n",
    "seq_length = 50 # Training : 50, Sampling : 1\n",
    "hidden_size = 128   # 히든 레이어의 노드 개수\n",
    "learning_rate = 0.002\n",
    "num_epochs = 2\n",
    "num_hidden_layers = 2\n",
    "grad_clip = 5   # Gradient Clipping에 사용할 임계값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dde702ac76d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TextLoader를 이용해서 데이터를 불러옵니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# 학습데이터에 포함된 모든 단어들을 나타내는 변수인 chars와 chars에 id를 부여해 dict 형태로 만든 vocab을 선언합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mchars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchars\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# TextLoader를 이용해서 데이터를 불러옵니다.\n",
    "data_loader = TextLoader(data_dir, batch_size, seq_length)\n",
    "# 학습데이터에 포함된 모든 단어들을 나타내는 변수인 chars와 chars에 id를 부여해 dict 형태로 만든 vocab을 선언합니다.\n",
    "chars = data_loader.chars \n",
    "vocab = data_loader.vocab\n",
    "vocab_size = data_loader.vocab_size # 전체 단어개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 인풋데이터와 타겟데이터, 배치 사이즈를 입력받기 위한 플레이스홀더를 설정합니다.\n",
    "input_data = tf.placeholder(tf.int32, shape=[None, None])  # input_data : [batch_size, seq_length])\n",
    "target_data = tf.placeholder(tf.int32, shape=[None, None]) # target_data : [batch_size, seq_length])\n",
    "state_batch_size = tf.placeholder(tf.int32, shape=[])      # Training : 50, Sampling : 1\n",
    "\n",
    "# RNN의 마지막 히든레이어의 출력을 소프트맥스 출력값으로 변환해주기 위한 변수들을 선언합니다.\n",
    "# hidden_size -> vocab_size\n",
    "softmax_w = tf.Variable(tf.random_normal(shape=[hidden_size, vocab_size]), dtype=tf.float32)\n",
    "softmax_b = tf.Variable(tf.random_normal(shape=[vocab_size]), dtype=tf.float32)\n",
    "\n",
    "# num_hidden_layers만큼 LSTM cell(히든레이어)를 선언합니다.\n",
    "cells = []\n",
    "for _ in range(0, num_hidden_layers):\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size)\n",
    "    cells.append(cell)\n",
    "\n",
    "# cell을 종합해서 RNN을 정의합니다.\n",
    "cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "# 인풋데이터를 변환하기 위한 Embedding Matrix를 선언합니다.\n",
    "# vocab_size -> hidden_size\n",
    "embedding = tf.Variable(tf.random_normal(shape=[vocab_size, hidden_size]), dtype=tf.float32)\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "# 초기 state 값을 0으로 초기화합니다.\n",
    "initial_state = cell.zero_state(state_batch_size, tf.float32)\n",
    "\n",
    "# 학습을 위한 tf.nn.dynamic_rnn을 선언합니다.\n",
    "# outputs : [batch_size, seq_length, hidden_size]\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, dtype=tf.float32)\n",
    "# ouputs을 [batch_size * seq_length, hidden_size]] 형태로 바꿉니다.\n",
    "output = tf.reshape(outputs, [-1, hidden_size])\n",
    "\n",
    "# 최종 출력값을 설정합니다.\n",
    "# logits : [batch_size * seq_length, vocab_size]\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross Entropy 손실 함수를 정의합니다. \n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=target_data))\n",
    "\n",
    "# 옵티마이저를 선언하고 옵티마이저에 Gradient Clipping을 적용합니다.\n",
    "# grad_clip(=5)보다 큰 Gradient를 5로 Clippin합니다.\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "# 세션을 열고 학습을 진행합니다.\n",
    "with tf.Session() as sess:\n",
    "    # 변수들에 초기값을 할당합니다.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        data_loader.reset_batch_pointer()\n",
    "        # 초기 상태값을 지정합니다.\n",
    "        state = sess.run(initial_state, feed_dict={state_batch_size : batch_size})\n",
    "\n",
    "        for b in range(data_loader.num_batches):\n",
    "            # x, y 데이터를 불러옵니다.\n",
    "            x, y = data_loader.next_batch()\n",
    "            # y에 one_hot 인코딩을 적용합니다. \n",
    "            y = tf.one_hot(y, vocab_size)            # y : [batch_size, seq_length, vocab_size]\n",
    "            y = tf.reshape(y, [-1, vocab_size])       # y : [batch_size * seq_length, vocab_size]\n",
    "            y = y.eval()\n",
    "\n",
    "            # feed-dict에 사용할 값들과 LSTM 초기 cell state(feed_dict[c])값과 hidden layer 출력값(feed_dict[h])을 지정합니다.\n",
    "            feed_dict = {input_data : x, target_data: y, state_batch_size : batch_size}\n",
    "            for i, (c, h) in enumerate(initial_state):\n",
    "                feed_dict[c] = state[i].c\n",
    "                feed_dict[h] = state[i].h\n",
    "\n",
    "            # 한스텝 학습을 진행합니다.\n",
    "            _, loss_print, state = sess.run([train_step, loss, final_state], feed_dict=feed_dict)\n",
    "\n",
    "            print(\"{}(학습한 배치개수)/{}(학습할 배치개수), 반복(epoch): {}, 손실함수(loss): {:.3f}\".format(\n",
    "                          e * data_loader.num_batches + b,\n",
    "                          num_epochs * data_loader.num_batches,\n",
    "                          (e+1), \n",
    "                          loss_print))\n",
    "\n",
    "    print(\"트레이닝이 끝났습니다!\")   \n",
    "    \n",
    "\n",
    "    # 샘플링 시작\n",
    "    print(\"샘플링을 시작합니다!\")\n",
    "    num_sampling = 4000  # 생성할 글자(Character)의 개수를 지정합니다. \n",
    "    prime = u' '         # 시작 글자를 ' '(공백)으로 지정합니다.\n",
    "    sampling_type = 1    # 샘플링 타입을 설정합니다.\n",
    "    state = sess.run(cell.zero_state(1, tf.float32)) # RNN의 최초 state값을 0으로 초기화합니다.\n",
    "\n",
    "    # Random Sampling을 위한 weighted_pick 함수를 정의합니다.\n",
    "    def weighted_pick(weights):\n",
    "        t = np.cumsum(weights)\n",
    "        s = np.sum(weights)\n",
    "        return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "    ret = prime       # 샘플링 결과를 리턴받을 ret 변수에 첫번째 글자를 할당합니다.\n",
    "    char = prime[-1]   # Char-RNN의 첫번쨰 인풋을 지정합니다.  \n",
    "    for n in range(num_sampling):\n",
    "        x = np.zeros((1, 1))\n",
    "        x[0, 0] = vocab[char]\n",
    "\n",
    "        # RNN을 한스텝 실행하고 Softmax 행렬을 리턴으로 받습니다.\n",
    "        feed_dict = {input_data: x, state_batch_size : 1, initial_state: state}\n",
    "        [probs_result, state] = sess.run([probs, final_state], feed_dict=feed_dict)         \n",
    "\n",
    "        # 불필요한 차원을 제거합니다.\n",
    "        # probs_result : (1,65) -> p : (65)\n",
    "        p = np.squeeze(probs_result)\n",
    "\n",
    "        # 샘플링 타입에 따라 3가지 종류로 샘플링 합니다.\n",
    "        # sampling_type : 0 -> 다음 글자를 예측할때 항상 argmax를 사용\n",
    "        # sampling_type : 1(defualt) -> 다음 글자를 예측할때 항상 random sampling을 사용\n",
    "        # sampling_type : 2 -> 다음 글자를 예측할때 이전 글자가 ' '(공백)이면 random sampling, 그렇지 않을 경우 argmax를 사용\n",
    "        if sampling_type == 0:\n",
    "            sample = np.argmax(p)\n",
    "        elif sampling_type == 2:\n",
    "            if char == ' ':\n",
    "                sample = weighted_pick(p)\n",
    "            else:\n",
    "                sample = np.argmax(p)\n",
    "        else:\n",
    "            sample = weighted_pick(p)\n",
    "\n",
    "        pred = chars[sample]\n",
    "        ret += pred     # 샘플링 결과에 현재 스텝에서 예측한 글자를 추가합니다. (예를들어 pred=L일 경우, ret = HEL -> HELL)\n",
    "        char = pred     # 예측한 글자를 다음 RNN의 인풋으로 사용합니다.\n",
    "\n",
    "    print(\"샘플링 결과:\")\n",
    "    print(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
